{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalebmes/CS376-Machine-Learning/blob/main/Homeworks/CS376_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mhuaIgkG65g"
      },
      "source": [
        "---\n",
        "# CS376 HW3: RNN, LSTM, Attention ***(Total score: 60)***\n",
        "\n",
        "(Note that this score is not the same score that will be used in computing your final HW grade. The score will be re-scaled considering other homeworks.)\n",
        "\n",
        "In this assignment, you will implement neural machine translation (NMT) models using:\n",
        "\n",
        "1. RNNs\n",
        "2. LSTMs and LSTMs with attention\n",
        "\n",
        "As in the previous assignments, you will see code blocks that look like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "pass\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "x = 3.7\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "Also, please remember:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook (except for Discuss/Analysis section). You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run.\n",
        "\n",
        "---\n",
        "\n",
        "**1. How to submit**\n",
        "* Submit **one** files to KLMS (You don't need to submit pdf file anymore):\n",
        "  - hw3_{student_ID}.ipynb, \n",
        "\n",
        "**2. Note**\n",
        "*   **Both ipynb and PDF files must contain executed log of all code blocks (after the last execution).**\n",
        "*   Your code should be reproducible. If we cannot reproduce, you will get penalty so please make sure everything works well before you submit.\n",
        "*   You are required to use the functions given in the notebook, do not use more advanced methods.\n",
        "\n",
        "\n",
        "**3. Grading**\n",
        "* Total score is **60pt**.\n",
        "* Code (25pt) -  We will check whether you implemented your codes correctly and it can reproduce the results you made.\n",
        "* Evaluation (25pt) - You can get the evalutation score whenever you pass the test cell.\n",
        "* Discussion (10pt) - It will be given if you experiment deeply enough with different condition and provide your reasonable explanation.\n",
        "\n",
        "---\n",
        "\n",
        "###**Change log**###\n",
        "\n",
        "2022.04.28)  \n",
        "  LSTM Encoder-Decoder:   LSTM encoder prediction -> LSTM decoder prediction\n",
        "\n",
        "\n",
        "2022.05.02)  \n",
        "  Test cell for TODO1,2: Annotation added. Minor code fix.\n",
        "\n",
        "2022.05.09)  \n",
        "  MLPAttention: Notation fixes. (vector v -> h)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before start, fill your information and run it.\n",
        "\n",
        "NAME = \"Kaleb Mesfin Asfaw\"\n",
        "STUDENT_ID = \"20200805\"\n",
        "asgn3_score = 0"
      ],
      "metadata": {
        "id": "pRHMSaFhWhOe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBri01t3G65h"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First let's import some libraries that will be useful in this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cj9OGJRtG65h"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw8x1Bw7G65i"
      },
      "source": [
        "Make sure you are using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XZJUDZ_1G65i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1d193c-8d5f-4b01-db1c-f35bf0fb4e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')\n",
        "  \n",
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8hm3JYHG65j"
      },
      "source": [
        "For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J34BwlpNG65j"
      },
      "outputs": [],
      "source": [
        "def download_if_not_exist(file_name):\n",
        "  \n",
        "  if not os.path.exists(file_name):\n",
        "    import urllib.request\n",
        "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
        "\n",
        "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
        "    \n",
        "  return file_name\n",
        "\n",
        "def read_raw(file_name):\n",
        "  file_name = download_if_not_exist(file_name)\n",
        "  \n",
        "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
        "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
        "  return raw_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j1Td3dLnG65k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1a1166-c286-4ddb-bf7d-ee36d05fba6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n",
            "Wait!\tAttends !\n",
            "Wait!\tAttendez !\n",
            "I see.\tJe comprends.\n"
          ]
        }
      ],
      "source": [
        "raw_text = read_raw('./data.zip')\n",
        "print(raw_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQaSXNNXG65k"
      },
      "source": [
        "Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rBiBxz8EG65k"
      },
      "outputs": [],
      "source": [
        "def preprocess_raw(text):\n",
        "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "  out = ''\n",
        "  for i, char in enumerate(text.lower()):\n",
        "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
        "      out += ' '\n",
        "    out += char\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmkUOc4gG65k"
      },
      "source": [
        "We further split the source-target pairs into a source list and a target list. We use word-level tokenization here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHP6ZhVGG65l"
      },
      "outputs": [],
      "source": [
        "def split_source_target(text, max_len):\n",
        "  source, target = [], []\n",
        "  for i, line in enumerate(text.split('\\n')):\n",
        "    if i > 5000: # we only use 5000 pairs of translation\n",
        "      break\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "      src_tokens = parts[0].split(' ')\n",
        "      tgt_tokens = parts[1].split(' ')\n",
        "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
        "        source.append(src_tokens)\n",
        "        target.append(tgt_tokens)\n",
        "  return source, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H1f90fA2G65l"
      },
      "outputs": [],
      "source": [
        "def prepare_data(raw_text, max_len=10000):\n",
        "  text = preprocess_raw(raw_text)\n",
        "  source, target = split_source_target(text, max_len)\n",
        "  return source, target\n",
        "\n",
        "source, target = prepare_data(raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaPsQixJG65l"
      },
      "source": [
        "Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9zv-uU07G65l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "9c535a76-a931-43b3-9c40-1781fd903cbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def len_dis(text):\n",
        "  lens = [len(line) for line in text]\n",
        "  len_counter = collections.Counter(lens)\n",
        "\n",
        "  lens = np.array(list(len_counter.keys()))\n",
        "  sort_idx = np.argsort(lens)\n",
        "  lens_sort = lens[sort_idx]\n",
        "  len_counts = np.array(list(len_counter.values()))\n",
        "  len_counts_sort = len_counts[sort_idx]\n",
        "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
        "  return p, lens_sort\n",
        "  \n",
        "src_p, src_lens_sort = len_dis(source)\n",
        "tgt_p, tgt_lens_sort = len_dis(target)\n",
        "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
        "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
        "plt.title('Cumulative Distribution of Sentence Length')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpAMl-RGG65l"
      },
      "source": [
        "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D2B-aT3qG65l"
      },
      "outputs": [],
      "source": [
        "# hyper-param\n",
        "MAX_LEN = 8\n",
        "MIN_FREQ = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBrMUj8aG65m"
      },
      "source": [
        "### Build Vocabulary\n",
        "\n",
        "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
        "- Dictionary word2index: Convert word string into index: \n",
        "- Dictionary index2word: Convert index into word string\n",
        "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
        "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary \n",
        "    \n",
        "There are 4 special tokens added in the vocabulary:\n",
        "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
        "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
        "- 'eos': end of sentence. This indicates the end of a sentence\n",
        "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1ZlC21-IG65m"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "  def __init__(self, name, tokens, min_freq):\n",
        "    self.name = name\n",
        "    self.index2word = {\n",
        "      0: 'pad',\n",
        "      1: 'bos',\n",
        "      2: 'eos',\n",
        "      3: 'unk'\n",
        "    }\n",
        "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "    self.num_word = 4\n",
        "    token_freq = collections.Counter(tokens)\n",
        "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
        "    self._build_vocab(tokens)\n",
        "    \n",
        "  def _build_vocab(self, tokens):\n",
        "    for token in tokens:\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.num_word\n",
        "        self.index2word[self.num_word] = token\n",
        "        self.num_word += 1\n",
        "        \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.word2index.get(tokens, self.word2index['unk'])\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSAfZ9VYG65m"
      },
      "source": [
        "### Build Dataset\n",
        "\n",
        "The dataset pipeline involves the following steps:\n",
        "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
        "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
        "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
        "- We use the Pytorch *DataLoader* API to build the dataset generator.\n",
        "\n",
        "For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uQzcjBZqG65m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefbd92f-0a32-44e7-edfd-d2a6e7001a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "Vocabulary size of source language: 433\n",
            "Vocabulary size of target language: 420\n",
            "Total number of sentence pairs: 4990\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(name, tokens, min_freq):\n",
        "  tokens = [token for line in tokens for token in line]\n",
        "  return Vocab(name, tokens, min_freq)\n",
        "\n",
        "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
        "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
        "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
        "  return vocab_src, vocab_tgt\n",
        "\n",
        "def pad(line, padding_token):\n",
        "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
        "\n",
        "def build_tensor(text, lang, is_source):\n",
        "  lines = [lang[line] for line in text]\n",
        "  if not is_source:\n",
        "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
        "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
        "  valid_len = (array != lang['pad']).sum(1)\n",
        "  return array, valid_len\n",
        "\n",
        "def load_data_nmt(batch_size=2):\n",
        "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
        "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
        "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
        "  train_data = torch.utils.data.TensorDataset(\n",
        "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "  print(train_data[0])\n",
        "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "  return lang_eng, lang_fra, train_iter\n",
        "\n",
        "\n",
        "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
        "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
        "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
        "print('Total number of sentence pairs: {}'.format(len(source)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZuEXonbG65m"
      },
      "source": [
        "## Sequence to Sequence with RNN (baseline)\n",
        "\n",
        "In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
        "\n",
        "Implementation Details:\n",
        "\n",
        "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
        "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
        "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
        "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
        "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBNAJ6eWG65n"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lsdusjPfG65n"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, sources, valid_len):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
        "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
        "    \"\"\"\n",
        "    word_embedded = self.embedding(sources)\n",
        "\n",
        "    N = word_embedded.shape[0]\n",
        "    \n",
        "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
        "    \n",
        "    o, h = self.enc(word_embedded, h)\n",
        "    \n",
        "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, h, target):\n",
        "    word_embedded = self.embedding(target)\n",
        "    N, T = word_embedded.shape[:2]\n",
        "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
        "    pred = self.output_emb(o)\n",
        "    return pred, h\n",
        "\n",
        "class NMTRNN(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
        "    super(NMTRNN, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
        "    \n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    h = self.enc(src, src_len)\n",
        "    T = tgt.shape[1]\n",
        "    pred, _ = self.dec(h, tgt)\n",
        "       \n",
        "    loss = F.nll_loss(F.log_softmax(pred[:, :T-1].transpose(1,2), dim = 1), tgt[:, 1:], ignore_index=0, reduction = 'none')\n",
        "    loss = loss.sum(1).mean()\n",
        "\n",
        "    return loss, pred.argmax(dim=-1)\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "      \"\"\"\n",
        "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
        "      token from the previous time step.\n",
        "      \"\"\"\n",
        "      h = self.enc(src, src_len)\n",
        "      \n",
        "      inputs = tgt[:, :1]\n",
        "      preds = []\n",
        "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
        "        pred, h = self.dec(h, inputs)\n",
        "        preds.append(pred)\n",
        "        inputs = pred.argmax(dim=-1)\n",
        "        \n",
        "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
        "      return pred\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vbyFT1DKG65n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7ed044-6736-414e-b4d7-73524e14f7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "iter 0 / 7800\tLoss:\t30.051929\n",
            "pred:\t tensor([146,  65, 349,  53,   3, 330,  52, 330, 330, 330])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t10.511122\n",
            "pred:\t tensor([14,  3,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([15, 89,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t9.733450\n",
            "pred:\t tensor([  3, 164,   3,  11,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 48,  49, 113, 282,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t8.342284\n",
            "pred:\t tensor([ 3,  3,  5,  3, 11,  2, 11,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([36,  3, 37,  3,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t6.555187\n",
            "pred:\t tensor([36,  3,  3, 11,  2,  5,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([36,  9, 88, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t6.749533\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t7.104589\n",
            "pred:\t tensor([ 14,  28,   3, 227,  37,   3,  11,   2,   5,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  28,  35, 227,  37,   3,  11,   2,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t5.891044\n",
            "pred:\t tensor([171, 342,   3,  11,   2,   5,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t6.436800\n",
            "pred:\t tensor([14, 79, 72, 11,  2, 11,  3, 11,  3, 11])\n",
            "\n",
            "tgt:\t tensor([ 14, 197,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t5.109266\n",
            "pred:\t tensor([ 14, 171,   3,  11,   2,   5,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([14, 70,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t6.205434\n",
            "pred:\t tensor([267, 203,  78,   3,  41,  24,   2,   5,   3,   3])\n",
            "\n",
            "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t5.708277\n",
            "pred:\t tensor([ 14, 171,   3,   3,   5,   2,   5,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  70,  17, 205,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t4.465404\n",
            "pred:\t tensor([ 3,  5,  2,  5,  5,  5, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t5.497901\n",
            "pred:\t tensor([ 14, 171,   3,  75,   3,  11,   2,   5,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 213,  75,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t4.004459\n",
            "pred:\t tensor([ 93, 203,  14, 171,   3,  24,   2,  24,   3,  24])\n",
            "\n",
            "tgt:\t tensor([176, 203,  14, 116,   3,  24,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t4.218427\n",
            "pred:\t tensor([14,  3, 11,  2, 11,  3,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([103, 385,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t4.397956\n",
            "pred:\t tensor([15,  3, 72,  3, 11,  2,  5, 88, 11,  3])\n",
            "\n",
            "tgt:\t tensor([ 15, 291,   9,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t4.588868\n",
            "pred:\t tensor([ 14, 116, 179,   3,  11,   2,   5,  88,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 305,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t3.556705\n",
            "pred:\t tensor([ 36, 316,  41, 139,  11,   2,   5,   3,  11,   3])\n",
            "\n",
            "tgt:\t tensor([315, 316,  41, 139,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t3.297014\n",
            "pred:\t tensor([ 3, 11,  2,  5,  3, 11, 11, 11, 11,  5])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t3.380059\n",
            "pred:\t tensor([ 15, 282,  11,   2,   5,  88,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([  3, 265,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t3.034994\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  5,  3, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t3.334937\n",
            "pred:\t tensor([  3,   6, 248,  11,   2,   5,  88,   5,   3,  11])\n",
            "\n",
            "tgt:\t tensor([  3,  72, 248,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t3.590140\n",
            "pred:\t tensor([176, 393, 355,  24,   2,  24,  24,  24,  24,   3])\n",
            "\n",
            "tgt:\t tensor([  3,  37, 355,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t3.726007\n",
            "pred:\t tensor([ 14, 108,  74,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14, 289,  74,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t2.837228\n",
            "pred:\t tensor([176,   3, 237,  24,   2,   5,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([176,   9, 237,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t3.500475\n",
            "pred:\t tensor([ 38, 116, 203,   3,  11,   2,   5,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 38, 419, 203,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t3.559257\n",
            "pred:\t tensor([90, 91, 24,  2, 24, 24, 24,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([ 90, 176,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t2.728256\n",
            "pred:\t tensor([  3, 207,   3,  11,   2,   5,   5,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([  3, 207,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t2.682977\n",
            "pred:\t tensor([150, 354,   5,   2,   5,   3,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t2.476798\n",
            "pred:\t tensor([122,  37,  11,   2,   5,   3,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([  3, 314,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.386943\n",
            "pred:\t tensor([ 38,  40,  21, 390,  11,   2,   5,  88,  11,   3])\n",
            "\n",
            "tgt:\t tensor([ 38,  40,  21, 390,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t3.183138\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t3.304249\n",
            "pred:\t tensor([267,   3,  24,   2,  24,  24,  24,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([267, 234,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t2.687942\n",
            "pred:\t tensor([52,  3, 11,  2, 11,  3,  3,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t2.315697\n",
            "pred:\t tensor([36, 40, 81,  3, 11,  2, 11,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([ 38,  78, 214,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t2.540252\n",
            "pred:\t tensor([48,  3, 37, 10, 11,  2,  5, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([48,  3, 37, 10, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t2.001208\n",
            "pred:\t tensor([ 3, 41, 11,  2,  5,  3,  3,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([  3, 161,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t2.437479\n",
            "pred:\t tensor([ 3,  5,  5,  2,  5,  3, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 19,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t2.360372\n",
            "pred:\t tensor([ 36,   3,  11,   2,   5, 292,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t1.751339\n",
            "pred:\t tensor([ 3,  5,  2,  5, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t2.737441\n",
            "pred:\t tensor([171, 341, 236,  11,   2,  11,   3,   3,   3,  11])\n",
            "\n",
            "tgt:\t tensor([ 92, 341, 236,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t2.141050\n",
            "pred:\t tensor([267,   3,  24,   2,   5, 148,  24,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267, 118,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t2.343958\n",
            "pred:\t tensor([ 14,  17, 114,  11,   2,   5,   3,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  17, 114,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t1.426110\n",
            "pred:\t tensor([ 14,  79,  28,  41,   3,  11,   2,   5, 292,   3])\n",
            "\n",
            "tgt:\t tensor([ 14,  79,  28,  41, 229,  11,   2,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t2.466043\n",
            "pred:\t tensor([267,   3,  24,   2,   5, 148,  11,   3,  11,  11])\n",
            "\n",
            "tgt:\t tensor([267, 124,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.663215\n",
            "pred:\t tensor([ 15, 204, 393,   3,  11,   2,  11,   3,   3,   3])\n",
            "\n",
            "tgt:\t tensor([ 15, 204, 393,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t1.494587\n",
            "pred:\t tensor([52,  3, 11,  2, 11,  3,  3,  3, 11, 11])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t1.847355\n",
            "pred:\t tensor([15,  3, 11,  2,  5,  3,  3,  3,  3, 11])\n",
            "\n",
            "tgt:\t tensor([15, 23, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t1.774521\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  5,  3,  3,  3,  3])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_rnn(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "      \n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
        "\n",
        "rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zy5VZYyG65o"
      },
      "source": [
        "### RNN Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vse6-u02G65o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d3ca33cd-3670-44a9-d7d6-4b99f156b33d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Baseline')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# save the loss curve figure in a file for the report\n",
        "rnn_loss_list = torch.tensor(rnn_loss_list, device = 'cpu')\n",
        "plt.plot(np.arange(len(rnn_loss_list)), rnn_loss_list)\n",
        "plt.title('Loss Curve of Baseline')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v1trnxiG65o"
      },
      "source": [
        "### Prediction Accuracy\n",
        "\n",
        "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 70%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FLdflv6XG65o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d729f21-0a69-4dd7-998a-c1dbb7edbdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['entrez', '!']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'unk', 'vu', '.']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'grand', '?']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', 'à', 'qui', 'que', 'ce', 'soit', '!']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.7096\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_rnn(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd == 'eos':\n",
        "          break\n",
        "          \n",
        "        pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  \n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_rnn(rnn_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baeu3Lk0G65o"
      },
      "source": [
        "## Sequence to Sequence with LSTM and Attention\n",
        "\n",
        "Now let's try to improve our model by using an LSTM and the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCcKIb1G65p"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
        "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
        "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
        "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
        "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
        "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Implement the LSTM class below. In particular,\n",
        "-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n",
        "- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "386a51799571153e76849c4b5ebb7f73",
          "grade": false,
          "grade_id": "cell-e43516618029ca06",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wO8PeoRDG65p"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, device):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.device = device\n",
        "    self.params = nn.ParameterList(self.init_params(input_size, hidden_size))\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      device: torch.device()\n",
        "    \"\"\"\n",
        "  \n",
        "  def init_params(self, input_size, hidden_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      \n",
        "    Outputs:\n",
        "      Weights for proposal: W_xc, W_hc, b_c\n",
        "      Weights for input gate: W_xi, W_hi, b_i\n",
        "      Weights for forget gate: W_xf, W_hf, b_f\n",
        "      Weights for output gate: W_xo, W_ho, b_o\n",
        "    \"\"\"\n",
        "    W_xc, W_hc, b_c = None, None, None\n",
        "    W_xi, W_hi, b_i = None, None, None\n",
        "    W_xf, W_hf, b_f = None, None, None\n",
        "    W_xo, W_ho, b_o = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO1: Initialize the weights and biases. The result will be stored in \n",
        "    # `params` below. Weights should be initialized using `torch.randn` multiplied \n",
        "    # with the scale (0.1). Biases should be initialized to 0.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    W_xc, W_hc, b_c = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1), nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1), nn.Parameter(torch.zeros(hidden_size))\n",
        "    W_xi, W_hi, b_i = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1), nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1), nn.Parameter(torch.zeros(hidden_size))\n",
        "    W_xf, W_hf, b_f = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1), nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1), nn.Parameter(torch.zeros(hidden_size))\n",
        "    W_xo, W_ho, b_o = nn.Parameter(torch.randn(input_size, hidden_size) * 0.1), nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1), nn.Parameter(torch.zeros(hidden_size))\n",
        "  \n",
        "    # END OF YOUR CODE\n",
        "    \n",
        "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
        "    return params\n",
        "\n",
        "  \n",
        "  def lstm(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
        "        T is the length of the sequence(s), D_in is the input size. src_len, size of (N,), is\n",
        "        the valid length for each sequence.\n",
        "        \n",
        "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n",
        "            (N, hidden_size), is the memory cell of the LSTM.\n",
        "      \n",
        "    Outputs:\n",
        "      o: tensor of size (N, T, hidden_size). Contains the output features (the hidden state H_t) for each t.\n",
        "      state: the same as input state. Contains the hidden state H_T and cell state C_T for the last timestep T.\n",
        "    \"\"\"\n",
        "    \n",
        "    src, src_len = X\n",
        "    h, c = state\n",
        "\n",
        "    # make sure always has a T dim\n",
        "    if len(src.shape) == 2:\n",
        "      src = src.unsqueeze(1)\n",
        "\n",
        "    N, T, D_in = src.shape\n",
        "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
        "    o = []\n",
        "    ##############################################################################\n",
        "    # TODO2: Implement the forward pass of the LSTM. LSTM must not be updated by\n",
        "    # unvalid inputs. In other words, you should update LSTM only when\n",
        "    # valid words come in. (use src_len)\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    hidden_size = h.shape[1]\n",
        "    o = torch.zeros((N, T, hidden_size), device=self.device)\n",
        "    for t in range(T):\n",
        "      x_t = src[:, t, :] # (N, D_in)\n",
        "      iden = torch.tensor([t] * N, device=self.device) - src_len.to(device)\n",
        "      iden[iden < 0] = -1\n",
        "      if -1 in iden:  \n",
        "        i_t = torch.sigmoid(x_t @ W_xi + h @ W_hi + b_i)\n",
        "        f_t = torch.sigmoid(x_t @ W_xf + h @ W_hf + b_f)\n",
        "        o_t = torch.sigmoid(x_t @ W_xo + h @ W_ho + b_o)\n",
        "        c_til = torch.tanh(x_t @ W_xc + h @ W_hc + b_c)\n",
        "        c = f_t * c + i_t * c_til\n",
        "        h = o_t * torch.tanh(c)\n",
        "      o[:, t, :] = h\n",
        "    \n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    state = (h, c)\n",
        "    return o, state\n",
        "  \n",
        "  def forward(self, inputs, state):\n",
        "    return self.lstm(inputs, state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYsrzoEG65p"
      },
      "source": [
        "Check that your output has the correct shape. You should see:\n",
        "\n",
        "```\n",
        "torch.Size([12, 8, 5])\n",
        "torch.Size([12, 5])\n",
        "torch.Size([12, 5])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZladPD0NG65p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2388a985-0d38-4f20-9e85-56869be5ee29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 8, 5])\n",
            "torch.Size([12, 5])\n",
            "torch.Size([12, 5])\n",
            "Correct\n"
          ]
        }
      ],
      "source": [
        "test_lstm = LSTM(10, 5, torch.device('cuda:0'))\n",
        "test_src = torch.ones(12, 8, 10)\n",
        "test_src[:,7] = 0\n",
        "test_src_len = torch.ones(12) * 7\n",
        "test_h = torch.zeros(12, 5).float()\n",
        "test_c = torch.zeros(12, 5).float()\n",
        "\n",
        "test_o, test_state = test_lstm((test_src, test_src_len), (test_h, test_c))\n",
        "\n",
        "print(test_o.shape)\n",
        "print(test_state[0].shape)\n",
        "print(test_state[1].shape)\n",
        "\n",
        "# must not be updated for the last element in this test case.\n",
        "if (test_o.shape == (12,8,5) and test_state[0].shape == (12,5) and test_state[1].shape == (12,5) and torch.equal(test_o[:,6], test_o[:,7])):\n",
        "  print(\"Correct\")\n",
        "  LSTM_score = 5\n",
        "\n",
        "else:\n",
        "  print(\"Wrong\")\n",
        "  LSTM_score = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2ddJSbVG65q"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_uB35eG65q"
      },
      "source": [
        "Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kywqkQslG65q"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oHnD3tuG65q"
      },
      "source": [
        "- **Attention**\n",
        "\n",
        "    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
        "    \n",
        "    \\begin{align*}\n",
        "    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n",
        "    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n",
        "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
        "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
        "    \\end{align*}\n",
        "    \n",
        "    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AtBw1NKG65q"
      },
      "source": [
        "- **Masked Softmax**\n",
        "\n",
        "    For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i85-MWhBG65q"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(X, valid_length):\n",
        "  \"\"\"\n",
        "  inputs:\n",
        "    X: 3-D tensor\n",
        "    valid_length: 1-D or 2-D tensor\n",
        "  \"\"\"\n",
        "  mask_value = -1e7 \n",
        "\n",
        "  if len(X.shape) == 2:\n",
        "    X = X.unsqueeze(1)\n",
        "\n",
        "  N, n, m = X.shape\n",
        "\n",
        "  if len(valid_length.shape) == 1:\n",
        "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
        "  else:\n",
        "    valid_length = valid_length.reshape((-1,))\n",
        "\n",
        "  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n",
        "  X.view(-1, m)[mask] = mask_value\n",
        "\n",
        "  Y = torch.softmax(X, dim=-1)\n",
        "\n",
        "  \n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z7ZHgxnbG65q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4540c7-96d9-4e66-e99a-246459511f19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4667, 0.5333, 0.0000, 0.0000],\n",
              "         [0.5474, 0.4526, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2324, 0.5569, 0.2107, 0.0000],\n",
              "         [0.3379, 0.4132, 0.2489, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzdNXsCqG65q"
      },
      "source": [
        "- **Scaled Dot Product Attention**\n",
        "    - The scaled dot-product attention uses the score function as: \n",
        "  $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n",
        "\n",
        "    <div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
        "\n",
        "\n",
        "Implement the DotProductAttention below. Do not use any loops in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b7a6c1703c6c230a006a6b86326a3b9",
          "grade": false,
          "grade_id": "cell-eac4fccbcd4f068e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xNC5x7VrG65q"
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module): \n",
        "  def __init__(self):\n",
        "      super(DotProductAttention, self).__init__()\n",
        "\n",
        "  def forward(self, query, key, value, valid_length=None):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO3: Implement the forward pass of DotProductAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    \n",
        "    attention = masked_softmax((query @ key.permute(0, 2, 1))/(key.shape[1] ** 0.5), valid_length) @ value\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzXKt6chG65r"
      },
      "source": [
        "### Correctness Check for DotProductAttention\n",
        "\n",
        "Run the following snippet to check your implementation of DotProductAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2hkj7oVXG65r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a80492-3f30-4693-cbd7-a9b908282a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct\n"
          ]
        }
      ],
      "source": [
        "att = DotProductAttention()\n",
        "keys = torch.ones((2,10,2),dtype=torch.float)\n",
        "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
        "res = att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n",
        "\n",
        "ans = torch.tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],[[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "\n",
        "if torch.all(torch.abs(ans - res) < 1e-03):\n",
        "  print(\"Correct\")\n",
        "  Dot_score = 5\n",
        "else:\n",
        "  print(\"Wrong\")\n",
        "  Dot_score = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WG37XqrG65r"
      },
      "source": [
        "- **MLP Attention**\n",
        "\n",
        "  In MLP attention, we project both query and keys into $R^{h}$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n",
        "\n",
        "    $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{h}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})$\n",
        "    \n",
        "    where $\\mathbf{h} \\in R^{h}, \\mathbf{W_k}\\text{, and }\\mathbf{W_q}$ are learnable parameters.\n",
        "    \n",
        "  Implement the MLP attention in matrix form without using any loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9440f3519ad5f8037f192758aecca64a",
          "grade": false,
          "grade_id": "cell-6be727894d4fd817",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "wIznAhcpG65r"
      },
      "outputs": [],
      "source": [
        "class MLPAttention(nn.Module):  \n",
        "  def __init__(self, d_h, d_k, d_q):\n",
        "    super(MLPAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_k: feature dimension of key\n",
        "      d_h: feature dimension of vector h\n",
        "      d_q: feature dimension of query\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO4: Initialize learnable parameters\n",
        "    # Use nn.Linear\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.w_k = nn.Linear(d_k, d_h, bias=False)\n",
        "    self.w_q = nn.Linear(d_q, d_h, bias=False)\n",
        "    self.v = nn.Linear(d_h, 1, bias=False)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d_q)\n",
        "      key: tensor of size (B, m, d_k)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: either (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d_q is the feature dimension of the query, d_k is the feature dimension of the key, \n",
        "      and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      Y: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO5: Implement the forward pass of MLPAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    learned_q = self.w_q(query).unsqueeze(2) # (B, n, 1, d_h)\n",
        "    learned_k = self.w_k(key).unsqueeze(1) # (B, 1, m, d_h)\n",
        "    features = torch.tanh(learned_q + learned_k) # (B, n, m, d_h)\n",
        "    learned_features = self.v(features).squeeze(-1) # (B, n, m, 1) -> (B, n, m) \n",
        "    Y = masked_softmax(learned_features, valid_length) @ value #and value -> (B, m, dim_v) so (B, n, dim_v)\n",
        "    # END OF YOUR CODE\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l47pb27G65r"
      },
      "source": [
        "### Correctness Check for MLPAttention\n",
        "\n",
        "Run the following snippet to check your implementation of MLPAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iqONoGz3G65r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8835cc-d71d-453c-e1fc-2b50e55a67a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct\n"
          ]
        }
      ],
      "source": [
        "atten = MLPAttention(4, 2, 2)\n",
        "res = atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6])).detach()\n",
        "\n",
        "ans = torch.tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],[[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "\n",
        "if torch.all(torch.abs(ans - res) < 1e-03):\n",
        "  print(\"Correct\")\n",
        "  MLP_score = 5\n",
        "\n",
        "else:\n",
        "  print(\"Wrong\")\n",
        "  MLP_score = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khJX-HWEG65r"
      },
      "source": [
        "    \n",
        "- **Using Attention in seq2seq Models**\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=18Z_FO69T-hS5XUltsrC4DDvoSM8LoY9y'>\n",
        "\n",
        "Image source: https://d2l.ai/_images/seq2seq-attention.svg\n",
        "\n",
        "Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n",
        "    \n",
        "In rough pseudocode, this looks like:\n",
        "\n",
        "    \n",
        "    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n",
        "    decoder_input = concatenate([decoder_input, context])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppM2pDCsG65r"
      },
      "source": [
        "### LSTM Encoder-Decoder\n",
        "\n",
        "\n",
        "Build a seq2seq model with LSTM and attention.\n",
        "\n",
        "- Complete the Encoder forward() function.\n",
        "- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n",
        "- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbd2ff0f838eab4eeb305bd07bbd3a8e",
          "grade": false,
          "grade_id": "cell-85d8bda82bc92dd8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "s_SKHSSkG65r"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim, hidden_size, device)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, sources, valid_len):\n",
        "    ##############################################################################\n",
        "    # TODO6: Implement LSTM Encoder forward pass\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    \n",
        "    word_embedded = self.embedding(sources) # (N, T, emb_dim)\n",
        "    N = word_embedded.shape[0]\n",
        "    # initialize the hidden state of LSTM\n",
        "    h = sources.new_zeros(N, self.hidden_size).float() # (N, hidden_size) \n",
        "    # initialize the memory state of LSTM\n",
        "    c = sources.new_zeros(N, self.hidden_size).float() # (N, hidden_size) \n",
        "    #forward propagation of LSTM -> inputs are (X, State)\n",
        "    outputs, (h, c) = self.enc((word_embedded, valid_len), (h, c))\n",
        "    # END OF YOUR CODE\n",
        "    return outputs, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8f15ad91df74611a4f70744a202ad53",
          "grade": false,
          "grade_id": "cell-154ce877082ed913",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kg-hp8UvG65s"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
        "    self.att = DotProductAttention()\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, state, target, valid_len):\n",
        "    loss = 0\n",
        "    preds = []\n",
        "    \n",
        "    ##############################################################################\n",
        "    # TODO7: Implement LSTM Decoder forward pass. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "\n",
        "    #getting outputs, hidden_state, and memory state of the encoder\n",
        "    # enc_outputs = (N, Tx, hidden_size) -> because it's not yet predicted\n",
        "    enc_outputs, (h, c), src_len = state # h = c = (N, hidden_size) \n",
        "    word_embedded = self.embedding(target) # (N, T, emb_dim)\n",
        "    N, T, emb_dim = word_embedded.shape\n",
        "    output_list = torch.zeros((N, T, self.hidden_size)).to(device) \n",
        "    for t in range(T): #word_embedded[:, t, :] -> (N, emb_dim) \n",
        "      x = word_embedded[:, t, :] # (N, emb_dim) -> (N, 1, emb_dim)\n",
        "      context = self.att(h.unsqueeze(1), enc_outputs, enc_outputs, valid_len) # (N, 1, hidden_size)\n",
        "      decoder_input = torch.cat([context, x.unsqueeze(1)], dim=2) # (N, 1, emb_dim+hidden_size)\n",
        "      outputs, (h, c) = self.enc((decoder_input, valid_len), (h, c)) #outputs -> (N, 1, hidden_size)\n",
        "      output_list[:, t, :] = outputs.squeeze(1) \n",
        "    preds = self.output_emb(output_list) # (N, T, vocab_size)\n",
        "    loss = F.nll_loss(F.log_softmax(preds[:, :T-1].transpose(1,2), dim = 1), target[:, 1:], ignore_index=0, reduction = 'none')\n",
        "    loss = loss.sum(1).mean()\n",
        "    preds = preds.argmax(dim=-1)\n",
        "    # END OF YOUR CODE\n",
        "    return loss, preds\n",
        "  \n",
        "  def predict(self, state, target, valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO8: Implement LSTM Decoder prediction. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    enc_outputs, (h, c), src_len = state\n",
        "    word_embedded = self.embedding(target)  #target -> (N, T, vocab_size) -> (N, T, emb_dim)\n",
        "    # #target -> (N, T), (N, vocab_size), (T, vocab_size)\n",
        "    (N, T) = target.shape\n",
        "    inputs = word_embedded[:, :1, :] #<bos> target #(N, 1, emb_dim)\n",
        "    preds = []\n",
        "    for t in range(T):\n",
        "      context = self.att(h.unsqueeze(1), enc_outputs, enc_outputs, valid_len) # (N, 1, hidden_size)\n",
        "      decoder_input = torch.cat([context, inputs], dim=2) # (N, 1, hidden_size+emb_dim)\n",
        "      outputs, (h, c) = self.enc((decoder_input, valid_len), (h, c)) #src_len/valid_len -> check\n",
        "      pred = self.output_emb(outputs) # (N, 1, hidden_size) -> (N, 1, vocab_size)\n",
        "      preds.append(pred)\n",
        "      inputs = self.embedding(pred.argmax(dim=-1))\n",
        "    pred = torch.cat(preds, dim=1).argmax(dim=-1) #(N, T, hidden_size) -> (N, T)\n",
        "    pred = pred[:, :-1]\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return pred\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-hKCKqQCG65s"
      },
      "outputs": [],
      "source": [
        "class NMTLSTM(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(NMTLSTM, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
        "    \n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return loss, pred\n",
        "  \n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73bc6101f369326d21d0efe8439c652b",
          "grade": false,
          "grade_id": "cell-bfaaa623c7199b2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mf0jjZ7GG65s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6687622-135d-464e-8e11-2930aff3f616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "iter 0 / 7800\tLoss:\t31.489624\n",
            "pred:\t tensor([281, 373, 204, 283, 205, 245,  66, 148,  69,  69])\n",
            "\n",
            "tgt:\t tensor([ 38, 338,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t12.765478\n",
            "pred:\t tensor([3, 5, 3, 3, 5, 2, 2, 5, 5, 5])\n",
            "\n",
            "tgt:\t tensor([ 97,   9,  74, 172,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t10.740396\n",
            "pred:\t tensor([ 3, 11,  2,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t7.361727\n",
            "pred:\t tensor([ 14,  79, 179,   3,  11,   2,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 134,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t6.462799\n",
            "pred:\t tensor([ 14, 171,   3,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 385, 230,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t5.485541\n",
            "pred:\t tensor([38, 78,  3, 11,  2,  2, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 38, 370,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t5.477722\n",
            "pred:\t tensor([198, 108,   3,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([290, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t4.356910\n",
            "pred:\t tensor([ 3, 74,  3,  5,  2,  5,  5,  5,  5,  5])\n",
            "\n",
            "tgt:\t tensor([  3,  74, 129,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t4.129749\n",
            "pred:\t tensor([ 15, 204,  37,   3,  11,   2,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 15, 204,  37,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t2.805463\n",
            "pred:\t tensor([249,  37, 413,   5,   2,   5,   5,   5,   5,   5])\n",
            "\n",
            "tgt:\t tensor([154,  37, 413,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t3.178627\n",
            "pred:\t tensor([ 36,  75,  45, 322,   5,   2,   5,   5,   5,   5])\n",
            "\n",
            "tgt:\t tensor([321,  75,  45, 322,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t2.726575\n",
            "pred:\t tensor([38,  3,  3, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([38, 91,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t2.642527\n",
            "pred:\t tensor([266,   3,  24,  24,  24,  24,  24,  24,  24,  24])\n",
            "\n",
            "tgt:\t tensor([266,   3,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t2.561343\n",
            "pred:\t tensor([155, 157,   3,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([156, 157, 323,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t1.905770\n",
            "pred:\t tensor([ 38, 116, 202, 147, 148,  11,   2,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 38, 116, 202, 112, 148,  11,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t2.083596\n",
            "pred:\t tensor([ 14,   3, 177,   3,  11,   2,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14,   3, 177,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t1.973900\n",
            "pred:\t tensor([14, 79, 28, 41,  3, 11,  2, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 14,  79,  28,  41, 221,  11,   2,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t1.687546\n",
            "pred:\t tensor([55,  3,  5,  2,  5,  5,  5,  5,  5, 11])\n",
            "\n",
            "tgt:\t tensor([55, 56,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t1.429765\n",
            "pred:\t tensor([168,  90,  79,   4,  41,  24,   2,  24,  24,  24])\n",
            "\n",
            "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t1.984988\n",
            "pred:\t tensor([  3, 314,  11,   2,  11,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([303, 314,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t1.506488\n",
            "pred:\t tensor([52, 40,  3, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([52, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t1.704130\n",
            "pred:\t tensor([167, 167,  45,  24,   2,  24,  24,  24,  24,  24])\n",
            "\n",
            "tgt:\t tensor([  9, 167,  45,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t2.133509\n",
            "pred:\t tensor([244, 320,  48,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([  3, 320,  48,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t1.466060\n",
            "pred:\t tensor([ 14, 116,  72,   3,  11,   2,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 116,  72,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t1.215745\n",
            "pred:\t tensor([14, 28, 54, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28, 54, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t1.293162\n",
            "pred:\t tensor([ 3,  5,  2,  5,  5,  5, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t1.149760\n",
            "pred:\t tensor([14, 28,  3, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t1.219050\n",
            "pred:\t tensor([14, 33, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t1.374923\n",
            "pred:\t tensor([38, 10, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([38,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t1.235743\n",
            "pred:\t tensor([ 14, 116, 179,   3, 259,  11,   2,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179,   3, 259,  11,   2,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t1.195179\n",
            "pred:\t tensor([15,  3, 75,  3, 11,  2, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([15,  3, 75,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t1.173899\n",
            "pred:\t tensor([14, 74,  3, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 15, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t1.002427\n",
            "pred:\t tensor([ 14, 116, 179, 259,  11,   2,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 259,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t1.139224\n",
            "pred:\t tensor([68, 11,  2, 11, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t1.011652\n",
            "pred:\t tensor([ 15, 204,  37,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([ 15, 204,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t1.183169\n",
            "pred:\t tensor([51,  9, 90, 90,  5,  2,  5,  5,  5,  5])\n",
            "\n",
            "tgt:\t tensor([51,  9,  3, 90,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t0.878750\n",
            "pred:\t tensor([170, 301,  24,   2,  24,  24,  24,  24,  24,  24])\n",
            "\n",
            "tgt:\t tensor([170,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t1.509874\n",
            "pred:\t tensor([ 3,  3, 81,  3, 24,  2, 24, 24, 24, 24])\n",
            "\n",
            "tgt:\t tensor([ 3, 71, 81,  3, 24,  2,  0,  0,  0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t1.269540\n",
            "pred:\t tensor([ 38,  40, 387, 110,   5,   2,   5,   5,   5,  11])\n",
            "\n",
            "tgt:\t tensor([ 38,  40, 102, 110,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t1.151498\n",
            "pred:\t tensor([55,  3,  5,  2,  5, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t1.248580\n",
            "pred:\t tensor([52,  3, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t1.327064\n",
            "pred:\t tensor([48,  3, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t1.000704\n",
            "pred:\t tensor([14, 33, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 33, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t0.900837\n",
            "pred:\t tensor([199,   3,   3,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([199, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t1.749870\n",
            "pred:\t tensor([108,   3,   3,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([108,   3,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t1.278892\n",
            "pred:\t tensor([ 3, 72,  3, 11,  2,  5, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([ 3, 74,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.116216\n",
            "pred:\t tensor([171, 342, 400,  11,   2,  11,  11,  11,  11,  11])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t0.825041\n",
            "pred:\t tensor([52, 40, 35, 11,  2, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([52, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t1.004784\n",
            "pred:\t tensor([36,  3, 11,  2, 11, 11, 11, 11, 11, 11])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t1.181507\n",
            "pred:\t tensor([176,  24,   2,   5,   5,   5,   5,   5,   5,  24])\n",
            "\n",
            "tgt:\t tensor([ 3, 24,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_lstm(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO9: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `lstm_net` variable.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "lr = 1e-3\n",
        "# END OF YOUR CODE\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
        "\n",
        "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3c_DR_6G65s"
      },
      "source": [
        "### LSTM Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "J1RubQaWG65s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "88684f83-365d-4713-e106-695884ea2946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of LSTM Attention')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "lstm_loss_list = torch.tensor(lstm_loss_list, device = 'cpu')\n",
        "plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n",
        "plt.title('Loss Curve of LSTM Attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHBh_lS-G65s"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 75% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LzwCs7cmG65s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c8fcb6-b004-453d-d73a-ffe0ec013858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['entrez', '!']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', \"t'en\", 'dois', 'une', '.']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'mort', '?']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['demande', 'à', 'qui', 'que', 'ce', 'soit', '!']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.7944\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_lstm(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "    \n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd == 'eos':\n",
        "          break\n",
        "        pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "  \n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  acc_final = torch.cat(acc_list).mean()\n",
        "  return acc_final\n",
        "  \n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "acc_final = evaluate_lstm(lstm_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if acc_final > 0.75:\n",
        "  print(\"Good!\")\n",
        "  Acc_score = 10\n",
        "else:\n",
        "  print(\"Try again!\")\n",
        "  Acc_score = 0"
      ],
      "metadata": {
        "id": "9lsIO57WKkYr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad1e784-a067-4412-bc81-60283268289a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total 25pt\n",
        "asgn3_score = LSTM_score + Dot_score + MLP_score + Acc_score\n",
        "asgn3_score"
      ],
      "metadata": {
        "id": "2kKlkHOoLebr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6deff4-12b3-4077-8aba-0eaa80509493"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss and Analysis\n",
        "\n",
        "In this section, you can experiment with whatever RNN architecture you'd like on this translation dataset and get higher accuracy as well as possible. You may try different architectures, hyperparameters, loss functions or other things. To get full credit, you should test at least one experiment.\n",
        "\n",
        "Also, you should describe what you did and analyze your experiment results for each experiment. (It can be your thoughts about the limitation of your work/ a possible direction for improvement/ reason why you failed to get high accuracy,..etc)\n",
        "\n",
        "### Things you might try:\n",
        "- **Hyperparameter**: Learning rate, Batch size, Vocabulary size, feature dimension, etc.\n",
        "- **Regularization**: Dropout, Batch normalization, etc\n",
        "- **Network architecture**: If you want, you can even use other than LSTM. (GRU,..)\n",
        "\n",
        "### Going above and beyond\n",
        "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
        "\n",
        "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
        "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
        "- Model ensembles\n",
        "- New Architectures\n",
        "\n",
        "Feel free to add code blocks and explanation cells below.\n"
      ],
      "metadata": {
        "id": "V0a-P2TnG_Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MYLSTM(nn.Module):\n",
        "  pass"
      ],
      "metadata": {
        "id": "O4OrraF6bXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WqXJKiMo0bn"
      },
      "source": [
        "**Experiment #1** (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout=0, isbidirectional=True):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the LSTM model\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # self.enc = LSTM(embedding_dim, hidden_size)\n",
        "    self.enc = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True, dropout=dropout, bidirectional=isbidirectional, num_layers=num_layers)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, sources, valid_len):    \n",
        "    word_embedded = self.embedding(sources) # (N, T, emb_dim)\n",
        "    N = word_embedded.shape[0]\n",
        "    h = sources.new_zeros(N, self.hidden_size).float() \n",
        "    c = sources.new_zeros(N, self.hidden_size).float() \n",
        "    outputs, (h, c) = self.enc((word_embedded, valid_len), (h, c))\n",
        "    return outputs, (h, c)\n",
        "  \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout=0, isbidirectional=True):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the LSTM model\n",
        "    \"\"\"\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # self.enc = LSTM(embedding_dim+hidden_size, hidden_size)\n",
        "    self.enc = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True, dropout=dropout, bidirectional=isbidirectional, num_layers=num_layers)\n",
        "    self.att = DotProductAttention()\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, state, target, valid_len):\n",
        "    \n",
        "    enc_outputs, (h, c), src_len = state # h = c = (N, hidden_size) \n",
        "    word_embedded = self.embedding(target) # (N, T, emb_dim)\n",
        "    N, T, emb_dim = word_embedded.shape\n",
        "    output_list = torch.zeros((N, T, self.hidden_size)).to(device) \n",
        "    for t in range(T): #word_embedded[:, t, :] -> (N, emb_dim) \n",
        "      x = word_embedded[:, t, :] # (N, emb_dim) -> (N, 1, emb_dim)\n",
        "      context = self.att(h.unsqueeze(1), enc_outputs, enc_outputs, valid_len) # (N, 1, hidden_size)\n",
        "      decoder_input = torch.cat([context, x.unsqueeze(1)], dim=2) # (N, 1, emb_dim+hidden_size)\n",
        "      outputs, (h, c) = self.enc((decoder_input, valid_len), (h, c)) #outputs -> (N, 1, hidden_size)\n",
        "      output_list[:, t, :] = outputs.squeeze(1) \n",
        "    preds = self.output_emb(output_list) # (N, T, vocab_size)\n",
        "    loss = F.nll_loss(F.log_softmax(preds[:, :T-1].transpose(1,2), dim = 1), target[:, 1:], ignore_index=0, reduction = 'none')\n",
        "    loss = loss.sum(1).mean()\n",
        "    preds = preds.argmax(dim=-1)\n",
        "    return loss, preds\n",
        "  \n",
        "  def predict(self, state, target, valid_len):\n",
        "    pred = None\n",
        "    \n",
        "    enc_outputs, (h, c), src_len = state\n",
        "    word_embedded = self.embedding(target)  #target -> (N, T, vocab_size) -> (N, T, emb_dim)\n",
        "    # #target -> (N, T), (N, vocab_size), (T, vocab_size)\n",
        "    (N, T) = target.shape\n",
        "    inputs = word_embedded[:, :1, :] #<bos> target #(N, 1, emb_dim)\n",
        "    preds = []\n",
        "    for t in range(T):\n",
        "      context = self.att(h.unsqueeze(1), enc_outputs, enc_outputs, valid_len) # (N, 1, hidden_size)\n",
        "      decoder_input = torch.cat([context, inputs], dim=2) # (N, 1, hidden_size+emb_dim)\n",
        "      outputs, (h, c) = self.enc((decoder_input, valid_len), (h, c)) #src_len/valid_len -> check\n",
        "      pred = self.output_emb(outputs) # (N, 1, hidden_size) -> (N, 1, vocab_size)\n",
        "      preds.append(pred)\n",
        "      inputs = self.embedding(pred.argmax(dim=-1))\n",
        "    pred = torch.cat(preds, dim=1).argmax(dim=-1) #(N, T, hidden_size) -> (N, T)\n",
        "    pred = pred[:, :-1]\n",
        "\n",
        "    return pred\n",
        "\n",
        "    \n",
        "\n",
        "class NMTLSTM(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
        "    super(NMTLSTM, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
        "    \n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return loss, pred\n",
        "  \n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "hd3lvk6re8TN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # 64, 128\n",
        "lr = 1e-3 \n",
        "epochs = 50 # may be early stopping\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
        "optimizer = torch.optim.Adam(lstm_net.parameters(), lr=lr)\n",
        "def train_lstm(net, train_iter, lr, epochs, optimizer, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr) # SGD optimizer\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "  \n",
        "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, optimizer, device)"
      ],
      "metadata": {
        "id": "CTxwu-U5e85z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "4eb31a87-ddb1-4711-8043-cd783d20b176"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5784ca33e67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvocab_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_fra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_nmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlstm_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMTLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_eng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_fra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 5 positional arguments but 6 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TdU9MPBFcGx"
      },
      "outputs": [],
      "source": [
        "lstm_loss_list = torch.tensor(lstm_loss_list, device = 'cpu')\n",
        "plt.plot(np.arange(len(lstm_loss_list)), lstm_loss_list)\n",
        "plt.title('Loss Curve of LSTM Attention')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "  \n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "  \n",
        "def evaluate_lstm(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "    \n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()] \n",
        "        if pred_wd == 'eos':\n",
        "          break\n",
        "        pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "  \n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "  acc_final = torch.cat(acc_list).mean()\n",
        "  return acc_final\n",
        "  \n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "acc_final = evaluate_lstm(lstm_net, train_iter, device)"
      ],
      "metadata": {
        "id": "JUmVw_Gjrhb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of CS376_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}