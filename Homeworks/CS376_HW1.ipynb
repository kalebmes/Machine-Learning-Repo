{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7InTcI1yCSk"
      },
      "source": [
        "# CS376 HW1 ***(Total score: 50)***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission Guideline"
      ],
      "metadata": {
        "id": "Rl4EvnTN8fQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. How to submit**\n",
        "*   Fill out <mark> TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
        "*   Submit two files: hw1_{student_ID}.ipynb, hw1_{student_ID}.pdf to KLMS\n",
        "\n",
        "    e.g. hw1_20221234.ipynb, hw1_20221234.pdf\n",
        "\n",
        "**2. Note**\n",
        "*   Both ipynb and PDF files must contain executed log of all code blocks (after the last execution).\n",
        "*   Your code should be reproducable. If we cannot reproduce, you will get penalty so please make sure everything works well before you submit.\n",
        "*   You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
        "*   There may be no default function to export the pdf file from the notebook in jupyter notebook or google colab. You can follow these steps:\n",
        "  ```\n",
        "    $pip install -U notebook-as-pdf\n",
        "    $pyppeteer-install\n",
        "    $jupyter-nbconvert --to PDFviaHTML <hw1_{student_ID}>.ipynb\n",
        "  ```"
      ],
      "metadata": {
        "id": "YMmWjluP8izk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHkyvNOhp-zM"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBPBZL98yWUn"
      },
      "source": [
        "The following is the basic preparation such as importing packages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sPO3mBFEZtF"
      },
      "outputs": [],
      "source": [
        "# For Python ≥3.5 \n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# For sklearn ≥0.20\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# common modules to import\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# For clean plotting\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Path to store images\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"training_linear_models\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"save plot:\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GphPqoWKyhut"
      },
      "source": [
        "Let's make our first dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wpFRzPyjpf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbBEZCemp-zQ"
      },
      "source": [
        "and plot it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceTO8RpBy82H"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "save_fig(\"generated_data_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgUiFyZBp-zS"
      },
      "source": [
        "## The Normal Equation ***(5 pts)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X7WwQB1p-zS"
      },
      "source": [
        "We first try to fit a linear model using the Normal Equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiXCuZVfzE2m"
      },
      "source": [
        "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHSmbZjp-zT"
      },
      "source": [
        "<mark>TODO-1</mark> Implement the Normal Equation in the following function using numpy. The function returns $\\hat{\\boldsymbol{\\theta}}$.  ***(5 pts)*** \n",
        "\n",
        "*Hint: You can use numpy function to avoid using loop to calculate **dot** product.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wwYW_zLzKL_"
      },
      "outputs": [],
      "source": [
        "def solve_normal_eq(X, y):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return theta_best"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "theta_best = solve_normal_eq(X_b, y)\n",
        "theta_best"
      ],
      "metadata": {
        "id": "E5sUA5bNG7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIaR-xBOp-zU"
      },
      "source": [
        "We can visualize the trained model as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYmiSRYs0VH1"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "y_predict\n",
        "X_new\n",
        "\n",
        "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "save_fig(\"linear_model_predictions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EojoyRLb0db0"
      },
      "source": [
        "## Gradient Descent Methods ***(5 pts)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRW_J-wj11P4"
      },
      "source": [
        "We can also fit the linear model by using the gradient descent method. The gradient of the MSE loss can be written as follows:\n",
        "\n",
        "$\n",
        "\\dfrac{\\partial}{\\partial \\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta})\n",
        " = \\dfrac{2}{m}\\sum_{i=1}^{m} (\\boldsymbol{\\theta}^T \\mathbf{x}^{(i)} - y^{(i)})x_j^{(i)}\n",
        " = \\dfrac{2}{m} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y})\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCRQ0g3Pp-zV"
      },
      "source": [
        "Then, the model parameter is updated as follows. Here, $\\eta$ is the learning rate.\n",
        "\n",
        "$\n",
        "\\boldsymbol{\\theta}^{(\\text{next step})} = \\boldsymbol{\\theta} - \\eta \\dfrac{\\partial}{\\partial \\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta})\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4csMFt-p-zW"
      },
      "source": [
        "<mark>TODO-2</mark> Implement the following function that updates the model parameter by gradient descent and returns the updated model parameter theta_next. ***(3 pts)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVbIAtJtp-zW"
      },
      "outputs": [],
      "source": [
        "def linear_mse_gradient_update(X, y, theta, m, eta):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return theta_next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KndMP2HJp-zW"
      },
      "source": [
        "The following function, plot_gradient_descent, is to visualize the first 10 models generated by gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUJgnpS316wR"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = []\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        theta = linear_mse_gradient_update(X_b, y, theta, m, eta)\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, 0, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO9CRD9ip-zX"
      },
      "source": [
        "<mark>TODO-3</mark> Try various learning rates and find one that converges well. ***(2 pts)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPutLyixp-zX"
      },
      "outputs": [],
      "source": [
        "######## TODO ########\n",
        "eta_best = ?\n",
        "######################\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plot_gradient_descent(theta, eta=eta_best, theta_path=theta_path_bgd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V82XbHs2L6T"
      },
      "source": [
        "## Mini-batch GD (Stochastic Gradient Descent) ***(15 pts)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDgfN3WMp-zX"
      },
      "source": [
        "Now we will train the same model using stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjoKphLc8Qfg"
      },
      "outputs": [],
      "source": [
        "n_iterations = 200\n",
        "minibatch_size = 20\n",
        "\n",
        "theta = np.random.randn(2,1)\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWhkvPlyp-zY"
      },
      "source": [
        "<mark>TODO-4</mark> Implement the following function, step_sgd(). ***(5 pts)*** \\\\\n",
        "The function should\n",
        "1. select a minibatch\n",
        "2. compute the gradient using the mini-batch\n",
        "3. update the parameter by the computed gradient and following the learning_schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtHIVvyUp-zY"
      },
      "outputs": [],
      "source": [
        "def step_sgd(X, y, theta, i, minibatch_size, eta):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    #######################\n",
        "    return theta "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiJfnnuOp-zY"
      },
      "source": [
        "<mark>TODO-5</mark> Implement the following function to shuffle the whole data once in an epoch. ***(5 pts)***\n",
        "\n",
        "Use np.random.permutation() to randomly shuffle indice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8q8tp39p-zY"
      },
      "outputs": [],
      "source": [
        "def shuffle_data(X_b, y):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return X_b_shuffled, y_shuffled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9JNYLa1p-zZ"
      },
      "source": [
        "<mark>TODO-6</mark> Using the above two functions implement the following fuction performing SGD updates for one epoch. ***(5 pts)***\n",
        "\n",
        "To make the remaining code work, add the following line whenever the parameter is updated.\n",
        "> theta_path_sgd.append(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm6OxCR6p-zZ"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(theta, X_b, y, m, minibatch_size, eta, theta_path_sgd):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return theta, theta_path_sgd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mprTJjtWp-zZ"
      },
      "source": [
        "Now you can train your model as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXUzs0NWp-zZ"
      },
      "outputs": [],
      "source": [
        "m = len(y)\n",
        "t = 0\n",
        "eta = 0.1\n",
        "theta_path_sgd = [] # to store the sgd update path\n",
        "theta = np.random.randn(2,1)\n",
        "\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "for epoch in range(n_iterations):\n",
        "    theta, theta_path_sgd = train_one_epoch(theta, X_b, y, m, minibatch_size, eta, theta_path_sgd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQ6F4XPp-zZ"
      },
      "source": [
        "Let's plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RKGgOoD8S2q"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "save_fig(\"gradient_descent_paths_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KObAL0r8pg3"
      },
      "source": [
        "## Polynomial Regresssion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28m2SRK9p-za"
      },
      "source": [
        "Let's make a new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi7t-YO18y19"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(45)\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHnPrRkb83RL"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"quadratic_data_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrEwjp5jp-zb"
      },
      "source": [
        "Using the sklearn package, let's extract more features of higher degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbJcHpTfBZwr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ZuM7hsp-zb"
      },
      "outputs": [],
      "source": [
        "X_poly[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpl5OSydp-zb"
      },
      "source": [
        "Let's fit the model using the LinearRegression package in sklearn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raysp-pRp-zb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF0NdFZXp-zc"
      },
      "outputs": [],
      "source": [
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"quadratic_predictions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx7RygyiEjZa"
      },
      "source": [
        "### Try degree = 10, 20, 30 and plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeOgF1B7p-zc"
      },
      "source": [
        "The previous example used only degree=2. Now let's try higher degrees and see how it overfits according to the degree. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEyhA9v0EzoG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "save_fig(\"high_degree_polynomials_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6lsd3GOp-zd"
      },
      "source": [
        "## Softmax Classification ***(25 pts)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96SAbRJ2p-zd"
      },
      "source": [
        "Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S48F63Ujp-zd"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F3qrkKOp-zd"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLBkrJhSp-zd"
      },
      "source": [
        "Add bias terms to all samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-8LIBo3p-zd"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # length and width as the features\n",
        "y = iris[\"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxhkLcbLp-ze"
      },
      "outputs": [],
      "source": [
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3kD-x80p-ze"
      },
      "source": [
        "<mark>TODO-7</mark> Split dataset into train, validation, and test sets. ***(3 pts)***\n",
        "\n",
        "Use np.random.permutation() to randomly shuffle indice and retrieve each dataset in order of train, validation and test."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, total_size, train_size, validation_size, test_size):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
      ],
      "metadata": {
        "id": "T3HJ-6BqA9Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok-RrWecp-ze"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2042)\n",
        "\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "X_train, y_train, X_valid, y_valid, X_test, y_test = train_test_split(X_with_bias, y, total_size, train_size, validation_size, test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ZMw4--p-ze"
      },
      "source": [
        "The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). \n",
        "\n",
        "<mark>TODO-8</mark> Write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance. ***(3 pts)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNdDCGY3p-zf"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y, n_classes):\n",
        "    ######## TODO ########\n",
        "    \n",
        "    ######################\n",
        "    return Y_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoR5IKxdp-zf"
      },
      "source": [
        "Looks good, so let's create the target class probabilities matrix for the training set and the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pSlE-X5p-zf"
      },
      "outputs": [],
      "source": [
        "Y_train_one_hot = to_one_hot(y_train, 3)\n",
        "Y_valid_one_hot = to_one_hot(y_valid, 3)\n",
        "Y_test_one_hot = to_one_hot(y_test, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rPfVm__p-zf"
      },
      "source": [
        "<mark>TODO-9</mark> Now let's implement the Softmax function. Recall that it is defined by the following equation: ***(3 pts)***\n",
        "\n",
        "\n",
        "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfHQPYxQp-zf"
      },
      "outputs": [],
      "source": [
        "def softmax(logits):\n",
        "    ####### TODO ########\n",
        "    \n",
        "    #####################\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7KTBqd5p-zg"
      },
      "source": [
        "We are almost ready to start training. Let's define the number of inputs and outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqfz3PMLp-zg"
      },
      "outputs": [],
      "source": [
        "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMKLeJBOp-zg"
      },
      "source": [
        "Now here comes the hardest part: training! Theoretically, it's simple: it's just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it's easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it's working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. \n",
        "\n",
        "So the equations we will need are the cost function:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
        "\n",
        "and its gradient w.r.t. the free parameters is:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
        "\n",
        "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So, you can add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemKnUwrp-zg"
      },
      "source": [
        "<mark>TODO-10</mark> Implement the following three functions: ***(3 pts each)***\n",
        "1. cross_entropy()\n",
        "2. gradient() \n",
        "3. predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LNZZlINp-zg"
      },
      "outputs": [],
      "source": [
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7      # for numerical stability of cross entropy loss\n",
        "\n",
        "class MySoftmaxClassifier():\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.theta = np.random.randn(n_inputs, n_outputs)\n",
        "        return\n",
        "    \n",
        "    def cross_entropy(self, Y_target, Y_pred):\n",
        "        ####### TODO #########\n",
        "        \n",
        "        ######################\n",
        "        return loss \n",
        "    \n",
        "    def gradient(self, Y_target, Y_pred, X_train):    \n",
        "        ####### TODO #########\n",
        "        \n",
        "        ######################\n",
        "        return gradients\n",
        "    \n",
        "    def predict(self, X):\n",
        "        ####### TODO #########\n",
        "        \n",
        "        ######################\n",
        "        return Y_proba\n",
        "        \n",
        "    def fit(self, X_train, Y_train_one_hot):\n",
        "        for iteration in range(n_iterations):\n",
        "            Y_proba = self.predict(X_train)\n",
        "            if iteration % 500 == 0:\n",
        "                loss = self.cross_entropy(Y_train_one_hot, Y_proba)\n",
        "                print(iteration, loss)\n",
        "            gradients = self.gradient(Y_train_one_hot, Y_proba, X_train)\n",
        "            self.theta = self.theta - eta * gradients\n",
        "            \n",
        "    def validation_score(self, X_valid, y_valid):\n",
        "        y_valid_one_hot = to_one_hot(y_valid, self.n_classes)\n",
        "        logits = X_valid.dot(self.theta)\n",
        "        Y_proba = softmax(logits)\n",
        "        Y_predict = np.argmax(Y_proba, axis=1)\n",
        "        accuracy_score = np.mean(Y_predict == y_valid)\n",
        "        \n",
        "        xent_loss = self.cross_entropy(y_valid_one_hot, Y_proba)\n",
        "        \n",
        "        return accuracy_score, xent_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFLDDO_8p-zh"
      },
      "outputs": [],
      "source": [
        "model = MySoftmaxClassifier(n_classes=3)\n",
        "model.fit(X_train, Y_train_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3fl59iFp-zh"
      },
      "source": [
        "And that's it! The Softmax model is trained. Let's look at the model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE_Ot-ccp-zh"
      },
      "outputs": [],
      "source": [
        "model.theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EruBGZKXp-zh"
      },
      "source": [
        "Let's make predictions for the validation set and check the accuracy score: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obbb2nh-p-zh"
      },
      "outputs": [],
      "source": [
        "model.validation_score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9pckHfpp-zh"
      },
      "source": [
        "Well, this model looks pretty good. For the sake of the exercise, let's add a bit of  ℓ2  regularization. \n",
        "\n",
        "The following class inherits the MySoftmaxClassifier you have already implemented. \\\\\n",
        "<mark>TODO-11</mark> Now, implement the cross_entropy() and gradient() function so that it has ℓ2 regularization. ***(2 pts each)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_dfhIyep-zi"
      },
      "outputs": [],
      "source": [
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7      # for numerical stability of cross entropy loss\n",
        "alpha = 0.1    # regularization hyperparameter\n",
        "\n",
        "class MySoftmaxClassifierL2(MySoftmaxClassifier):\n",
        "    def __init__(self, n_classes):\n",
        "        MySoftmaxClassifier.__init__(self, n_classes)\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        return \n",
        "    \n",
        "    def cross_entropy(self, Y_target, Y_pred):\n",
        "        ####### TODO #########\n",
        "        \n",
        "        ######################\n",
        "        return loss \n",
        "    \n",
        "    def gradient(self, Y_target, Y_pred, X_train):\n",
        "        ####### TODO #########\n",
        "        \n",
        "        ######################\n",
        "        return gradients"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MySoftmaxClassifierL2(n_classes=3)\n",
        "model.fit(X_train, Y_train_one_hot)"
      ],
      "metadata": {
        "id": "VC4ykHpQ7AdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9HzJA-Xp-zi"
      },
      "source": [
        "We see that the validation accuracy is improved with the regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRcjG8jRp-zi"
      },
      "outputs": [],
      "source": [
        "model.validation_score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKE9pGM_p-zi"
      },
      "source": [
        "<mark>TODO-12</mark> Implement early stopping in the following class. ***(3 pts)***\n",
        "\n",
        "Early stopping finishes training if the validation loss increases. Print the following when early stopping happens\n",
        "> print(iteration, val_loss, \"early stopping!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EIV2j9Up-zi"
      },
      "outputs": [],
      "source": [
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1    # regularization hyperparameter\n",
        "\n",
        "class MySoftmaxClassifierES(MySoftmaxClassifierL2):\n",
        "    def __init__(self, n_classes):\n",
        "        MySoftmaxClassifierL2.__init__(self, n_classes)\n",
        "        self.best_loss = np.infty\n",
        "        return \n",
        "    \n",
        "    def fit(self, X_train, Y_train_one_hot, X_valid, y_valid):\n",
        "        for iteration in range(n_iterations):\n",
        "            Y_proba = self.predict(X_train)\n",
        "            gradients = self.gradient(Y_train_one_hot, Y_proba, X_train)\n",
        "            self.theta = self.theta - eta * gradients\n",
        "            _, val_loss = self.validation_score(X_valid, y_valid)\n",
        "\n",
        "            if iteration % 500 == 0:\n",
        "                print(iteration, val_loss)\n",
        "                \n",
        "            ###### TODO ########\n",
        "            \n",
        "            #####################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MySoftmaxClassifierES(n_classes=3)\n",
        "model.fit(X_train, Y_train_one_hot, X_valid, y_valid)"
      ],
      "metadata": {
        "id": "71HWPDmp7IiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcoB7YkJp-zj"
      },
      "source": [
        "With early stopping, we see similar performance while training a smaller number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4DEs8ap-zj"
      },
      "outputs": [],
      "source": [
        "model.validation_score(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "_pApNf1d2Yua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluation Cell ### \n",
        "### The execution result of this cell has to be in the submission. But, never change this cell.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(376)\n",
        "eval_eta = 1\n",
        "eval_minibatch_size=3\n",
        "eval_theta_path_sgd = []\n",
        "\n",
        "eval_X = 2 * np.random.rand(10, 1)\n",
        "eval_y = 4 + 3 * eval_X + np.random.randn(10, 1)\n",
        "\n",
        "eval_X_b = np.c_[np.ones((len(eval_X), 1)), eval_X]\n",
        "todo_1 = solve_normal_eq(eval_X_b, eval_y)\n",
        "print(\"TODO#1 Result: \\n\", todo_1)\n",
        "\n",
        "eval_theta = np.random.randn(2,1)\n",
        "todo_2 = linear_mse_gradient_update(eval_X_b, eval_y, eval_theta, len(X), 1)\n",
        "print(\"\\nTODO#2 Result: \\n\", todo_2)\n",
        "\n",
        "todo_4 = step_sgd(eval_X_b, eval_y, eval_theta, 0, 3, eval_eta)\n",
        "print(\"\\nTODO#4 Result: \\n\", todo_4)\n",
        "\n",
        "todo_5, _ = shuffle_data(eval_X, eval_y)\n",
        "print(\"\\nTODO#5 Result: \\n\", todo_5[:5])\n",
        "\n",
        "todo_6, _ = train_one_epoch(theta, eval_X_b, eval_y, len(eval_y), 3, eval_eta, eval_theta_path_sgd)\n",
        "print(\"\\nTODO#6 Result: \\n\", todo_6)\n",
        "\n",
        "eval_test_ratio = 0.2\n",
        "eval_validation_ratio = 0.2\n",
        "eval_total_size = len(eval_X_b)\n",
        "\n",
        "eval_test_size = int(eval_total_size * eval_test_ratio)\n",
        "eval_validation_size = int(eval_total_size * eval_validation_ratio)\n",
        "eval_train_size = eval_total_size - eval_test_size - eval_validation_size\n",
        "\n",
        "eval_X_train, eval_y_train, eval_X_valid, eval_y_valid, _, todo_7 = train_test_split(eval_X_b, eval_y, eval_total_size, eval_train_size, eval_validation_size, eval_test_size)\n",
        "print(\"\\nTODO#7 Result: \\n\", todo_7)\n",
        "print(\"\\nTODO#7 Result: train len: {}, val len: {}, test len: {}\\n\".format(len(eval_X_train), len(eval_X_valid), len(todo_7)))\n",
        "\n",
        "todo_8 = to_one_hot(np.array([0, 1, 2, 5, 4, 3]), n_classes=6)\n",
        "print(\"\\nTODO#8 Result: \\n\", todo_8)\n",
        "\n",
        "todo_9 = softmax(np.array([[1, 2, 3, 4], [-1, -2, -3, -1]]))\n",
        "print(\"\\nTODO#9 Result: \\n\", todo_9)\n",
        "\n",
        "classification_model = MySoftmaxClassifier(n_classes=6)\n",
        "classification_model.theta = np.random.randn(2, 6)\n",
        "eval_y_proba = classification_model.predict(eval_X_train)\n",
        "eval_l = classification_model.cross_entropy(todo_8, eval_y_proba)\n",
        "eval_grad = classification_model.gradient(todo_8, eval_y_proba, eval_X_train)\n",
        "\n",
        "print(\"\\nTODO#10-1 Result: \\n\", eval_y_proba)\n",
        "print(\"\\nTODO#10-2 Result: \\n\", eval_l)\n",
        "print(\"\\nTODO#10-3 Result: \\n\", eval_grad)\n",
        "\n",
        "l2_classification_model = MySoftmaxClassifierL2(n_classes=6)\n",
        "l2_classification_model.theta = np.random.randn(2, 6)\n",
        "l2_classification_model.n_outputs = 6\n",
        "eval_y_proba = l2_classification_model.predict(eval_X_train)\n",
        "eval_l = l2_classification_model.cross_entropy(todo_8, eval_y_proba)\n",
        "eval_grad = l2_classification_model.gradient(todo_8, eval_y_proba, eval_X_train)\n",
        "\n",
        "print(\"\\nTODO#11-1 Result: \\n\", eval_y_proba)\n",
        "print(\"\\nTODO#11-2 Result: \\n\", eval_l)\n",
        "print(\"\\nTODO#11-3 Result: \\n\", eval_grad)\n",
        "\n",
        "print(\"\\nTODO#12 Result: \\n\")\n",
        "\n",
        "es_model = MySoftmaxClassifierES(n_classes=6)\n",
        "es_model.theta = np.random.randn(2, 6)\n",
        "es_model.n_outputs = 6\n",
        "es_model.fit(eval_X_train, todo_8, eval_X_valid, np.array([3, 4]))"
      ],
      "metadata": {
        "id": "bQlYbZXo2o1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fIEcis5WZW80"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS376_HW1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}